# Useful links  

## Operationalize AI Responsibly  

### Governance  
    Establish policies, practices, and processes that align with Microsoft's Responsible AI Standard. It includes embedding AI principles into the development lifecycle and workflows to comply with laws and regulations across privacy, security, and responsible AI.  
- [Microsoft Responsible AI Standard, v2](https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-Responsible-AI-Standard-v2-General-Requirements-3.pdf)
- [Microsoft Purview data security and compliance protections for Microsoft Copilot and other generative AI apps](https://learn.microsoft.com/en-us/purview/ai-microsoft-purview?source=docs%3Fwt.mc_id%3Dstudentamb_272081&ns-enrollment-type=Collection&ns-enrollment-id=5dj3hoep3n88gk)
### Red Team
    What is red teaming?
    The term red teaming has historically described systematic adversarial attacks for testing security vulnerabilities. With the rise of LLMs, the term has extended beyond traditional cybersecurity and evolved in common usage to describe many kinds of probing, testing, and attacking of AI systems. With LLMs, both benign and adversarial usage can produce potentially harmful outputs, which can take many forms, including harmful content such as hate speech, incitement or glorification of violence, or sexual content.  
- [Planning red teaming for large language models (LLMs) and their applications - Azure OpenAI Service](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming?source=docs%3Fwt.mc_id%3Dstudentamb_272081&ns-enrollment-type=Collection&ns-enrollment-id=5dj3hoep3n88gk)
- [PyRIT](https://github.com/Azure/PyRIT)
### Measure
    The frequency and severity of potential harms are measured using clear metrics, test sets, and systematic testing. This helps in understanding the trade-offs between different kinds of errors and experiences.  
- [Evaluate Flows with Prompty](https://microsoft.github.io/promptflow/tutorials/prompty-quickstart.html#evaluate-your-flow)  
- [Train a model and debug it with Responsible AI dashboard](https://learn.microsoft.com/en-us/training/modules/train-model-debug-with-responsible-ai-dashboard-azure-machine-learning/?source=docs%3Fwt.mc_id%3Dstudentamb_272081&ns-enrollment-type=Collection&ns-enrollment-id=5dj3hoep3n88gk)  
- [How to manually evaluate prompts in Azure AI Studio playground - Azure AI Studio](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/evaluate-prompts-playground?source=docs%3Fwt.mc_id%3Dstudentamb_272081&ns-enrollment-type=Collection&ns-enrollment-id=5dj3hoep3n88gk)  
- [How to generate adversarial simulations for safety evaluation - Azure AI Studio](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/simulator-interaction-data?source=docs%3Fwt.mc_id%3Dstudentamb_272081&ns-enrollment-type=Collection&ns-enrollment-id=5dj3hoep3n88gk)

### Mitigate  
    Mitigations are implemented using strategies like prompt engineering, grounding, and content filters. The effectiveness of these mitigations is then tested through manual and automated evaluations.  
    
- [Azure AI Studio content filtering - Azure AI Studio](https://learn.microsoft.com/en-us/azure/ai-studio/concepts/content-filtering?source=docs%3Fwt.mc_id%3Dstudentamb_272081&ns-enrollment-type=Collection&ns-enrollment-id=5dj3hoep3n88gk)  
- [Prompt Shields in Azure AI Studio (preview) - Azure AI services](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/prompt-shields?source=docs%3Fwt.mc_id%3Dstudentamb_272081&ns-enrollment-type=Collection&ns-enrollment-id=5dj3hoep3n88gk)  
- [Groundedness detection in Azure AI Studio(preview) - Azure AI services](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/groundedness?source=docs%3Fwt.mc_id%3Dstudentamb_272081&ns-enrollment-type=Collection&ns-enrollment-id=5dj3hoep3n88gk)  
- [System message framework and template recommendations for Large Language Models(LLMs) - Azure OpenAI Service](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/system-message?source=docs%3Fwt.mc_id%3Dstudentamb_272081&ns-enrollment-type=Collection&ns-enrollment-id=5dj3hoep3n88gk)  
- [Moderate Content and Detect Harm with Azure AI Content Safety](https://learn.microsoft.com/en-us/training/modules/moderate-content-detect-harm-azure-ai-content-safety/?source=docs%3Fwt.mc_id%3Dstudentamb_272081&ns-enrollment-type=Collection&ns-enrollment-id=5dj3hoep3n88gk)  
- [Moderate content and detect harm with Azure AI Content Safety Studio](https://learn.microsoft.com/en-us/training/modules/moderate-content-detect-harm-azure-ai-content-safety-studio/?source=docs%3Fwt.mc_id%3Dstudentamb_272081&ns-enrollment-type=Collection&ns-enrollment-id=5dj3hoep3n88gk)  

### Operate
    Define and execute a deployment and operational readiness plan, setup monitoring, and improve the AI application in production through monitoring and incident response.  
- [Monitor quality and token usage of deployed prompt flow applications (preview) - Azure AI Studio](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/monitor-quality-safety?source=docs%3Fwt.mc_id%3Dstudentamb_272081&ns-enrollment-type=Collection&ns-enrollment-id=5dj3hoep3n88gk)  

